{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 22:10:52.180192: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-24 22:10:52.368875: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-24 22:10:53.105872: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/keep9oing/DRQN-Pytorch-CartPole-v1\n",
    "\n",
    "import sys\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import FlattenObservation\n",
    "import collections\n",
    "import random\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from environment import PNDEnv\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_adjacency_matrix(n: int, density: float) -> np.ndarray:\n",
    "    \"\"\"Make adjacency matrix of a clique network.\n",
    "    \n",
    "    Args:\n",
    "        n (int): Number of nodes.\n",
    "        density (float): Density of the clique network.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Adjacency matrix.\n",
    "    \"\"\"\n",
    "    if density < 0 or density > 1:\n",
    "        raise ValueError(\"Density must be between 0 and 1.\")\n",
    "    \n",
    "    n_edges = int(n * (n - 1) / 2 * density)\n",
    "    adjacency_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        adjacency_matrix[i-1, i] = 1\n",
    "        adjacency_matrix[i, i-1] = 1\n",
    "        n_edges -= 1\n",
    "    \n",
    "    # If the density of the current adjacency matrix is over density, return it.\n",
    "    if n_edges <= 0:\n",
    "        return adjacency_matrix\n",
    "    else:\n",
    "        arr = [1]*n_edges + [0]*((n-1)*(n-2)//2 - n_edges)\n",
    "        np.random.shuffle(arr)\n",
    "        for i in range(0, n):\n",
    "            for j in range(i+2, n):\n",
    "                adjacency_matrix[i, j] = arr.pop()\n",
    "                adjacency_matrix[j, i] = adjacency_matrix[i, j]\n",
    "    return adjacency_matrix\n",
    "\n",
    "def show_graph_with_labels(adjacency_matrix):\n",
    "    rows, cols = np.where(adjacency_matrix == 1)\n",
    "    edges = zip(rows.tolist(), cols.tolist())\n",
    "    gr = nx.Graph()\n",
    "    gr.add_edges_from(edges)\n",
    "    pos = nx.kamada_kawai_layout(gr)\n",
    "    nx.draw(gr, pos=pos, with_labels=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q_network\n",
    "class Q_net(nn.Module):\n",
    "    def __init__(self, state_space=None,\n",
    "                 action_space=None):\n",
    "        super(Q_net, self).__init__()\n",
    "        # space size check\n",
    "        assert state_space is not None, \"None state_space input: state_space should be selected.\"\n",
    "        assert action_space is not None, \"None action_space input: action_space should be selected.\"\n",
    "\n",
    "        self.hidden_space = 64\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "\n",
    "        self.Linear1 = nn.Linear(self.state_space, self.hidden_space)\n",
    "        self.lstm    = nn.LSTM(self.hidden_space, self.hidden_space, batch_first=True)\n",
    "        self.Linear2 = nn.Linear(self.hidden_space, self.action_space)\n",
    "\n",
    "    def forward(self, x, h, c):\n",
    "        x = F.relu(self.Linear1(x))\n",
    "        x, (new_h, new_c) = self.lstm(x,(h,c))\n",
    "        x = self.Linear2(x)\n",
    "        return x, new_h, new_c\n",
    "\n",
    "    def sample_action(self, obs, h,c, epsilon):\n",
    "        output = self.forward(obs, h,c)\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0,1), output[1], output[2]\n",
    "        else:\n",
    "            return output[0].argmax().item(), output[1] , output[2]\n",
    "    \n",
    "    def init_hidden_state(self, batch_size, training=None):\n",
    "        assert training is not None, \"training step parameter should be dtermined\"\n",
    "        if training is True:\n",
    "            return torch.zeros([1, batch_size, self.hidden_space]), torch.zeros([1, batch_size, self.hidden_space])\n",
    "        else:\n",
    "            return torch.zeros([1, 1, self.hidden_space]), torch.zeros([1, 1, self.hidden_space])\n",
    "\n",
    "class EpisodeMemory():\n",
    "    \"\"\"Episode memory for recurrent agent\"\"\"\n",
    "    def __init__(self, random_update=False, \n",
    "                       max_epi_num=100, max_epi_len=500,\n",
    "                       batch_size=1,\n",
    "                       lookup_step=None):\n",
    "        self.random_update = random_update # if False, sequential update\n",
    "        self.max_epi_num = max_epi_num\n",
    "        self.max_epi_len = max_epi_len\n",
    "        self.batch_size = batch_size\n",
    "        self.lookup_step = lookup_step\n",
    "\n",
    "        if (random_update is False) and (self.batch_size > 1):\n",
    "            sys.exit('It is recommend to use 1 batch for sequential update, if you want, erase this code block and modify code')\n",
    "\n",
    "        self.memory = collections.deque(maxlen=self.max_epi_num)\n",
    "\n",
    "    def put(self, episode):\n",
    "        self.memory.append(episode)\n",
    "\n",
    "    def sample(self):\n",
    "        sampled_buffer = []\n",
    "\n",
    "        ##################### RANDOM UPDATE ############################\n",
    "        if self.random_update: # Random update\n",
    "            sampled_episodes = random.sample(self.memory, self.batch_size)\n",
    "            \n",
    "            check_flag = True # check if every sample data to train is larger than batch size\n",
    "            min_step = self.max_epi_len\n",
    "\n",
    "            for episode in sampled_episodes:\n",
    "                min_step = min(min_step, len(episode)) # get minimum step from sampled episodes\n",
    "\n",
    "            for episode in sampled_episodes:\n",
    "                if min_step > self.lookup_step: # sample buffer with lookup_step size\n",
    "                    idx = np.random.randint(0, len(episode)-self.lookup_step+1)\n",
    "                    sample = episode.sample(random_update=self.random_update, lookup_step=self.lookup_step, idx=idx)\n",
    "                    sampled_buffer.append(sample)\n",
    "                else:\n",
    "                    idx = np.random.randint(0, len(episode)-min_step+1) # sample buffer with minstep size\n",
    "                    sample = episode.sample(random_update=self.random_update, lookup_step=min_step, idx=idx)\n",
    "                    sampled_buffer.append(sample)\n",
    "\n",
    "        ##################### SEQUENTIAL UPDATE ############################           \n",
    "        else: # Sequential update\n",
    "            idx = np.random.randint(0, len(self.memory))\n",
    "            sampled_buffer.append(self.memory[idx].sample(random_update=self.random_update))\n",
    "\n",
    "        return sampled_buffer, len(sampled_buffer[0]['obs']) # buffers, sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class EpisodeBuffer:\n",
    "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.obs = []\n",
    "        self.action = []\n",
    "        self.reward = []\n",
    "        self.next_obs = []\n",
    "        self.done = []\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.obs.append(transition[0])\n",
    "        self.action.append(transition[1])\n",
    "        self.reward.append(transition[2])\n",
    "        self.next_obs.append(transition[3])\n",
    "        self.done.append(transition[4])\n",
    "\n",
    "    def sample(self, random_update=False, lookup_step=None, idx=None) -> Dict[str, np.ndarray]:\n",
    "        obs = np.array(self.obs)\n",
    "        action = np.array(self.action)\n",
    "        reward = np.array(self.reward)\n",
    "        next_obs = np.array(self.next_obs)\n",
    "        done = np.array(self.done)\n",
    "\n",
    "        if random_update is True:\n",
    "            obs = obs[idx:idx+lookup_step]\n",
    "            action = action[idx:idx+lookup_step]\n",
    "            reward = reward[idx:idx+lookup_step]\n",
    "            next_obs = next_obs[idx:idx+lookup_step]\n",
    "            done = done[idx:idx+lookup_step]\n",
    "\n",
    "        return dict(obs=obs,\n",
    "                    acts=action,\n",
    "                    rews=reward,\n",
    "                    next_obs=next_obs,\n",
    "                    done=done)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.obs)\n",
    "\n",
    "\n",
    "def train(q_net=None, target_q_net=None, episode_memory=None,\n",
    "          device=None, \n",
    "          optimizer = None,\n",
    "          batch_size=1,\n",
    "          learning_rate=1e-3,\n",
    "          gamma=0.99):\n",
    "\n",
    "    assert device is not None, \"None Device input: device should be selected.\"\n",
    "\n",
    "    # Get batch from replay buffer\n",
    "    samples, seq_len = episode_memory.sample()\n",
    "\n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_observations = []\n",
    "    dones = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        observations.append(samples[i][\"obs\"])\n",
    "        actions.append(samples[i][\"acts\"])\n",
    "        rewards.append(samples[i][\"rews\"])\n",
    "        next_observations.append(samples[i][\"next_obs\"])\n",
    "        dones.append(samples[i][\"done\"])\n",
    "\n",
    "    observations = np.array(observations)\n",
    "    actions = np.array(actions)\n",
    "    rewards = np.array(rewards)\n",
    "    next_observations = np.array(next_observations)\n",
    "    dones = np.array(dones)\n",
    "\n",
    "    observations = torch.FloatTensor(observations.reshape(batch_size,seq_len,-1)).to(device)\n",
    "    actions = torch.LongTensor(actions.reshape(batch_size,seq_len,-1)).to(device)\n",
    "    rewards = torch.FloatTensor(rewards.reshape(batch_size,seq_len,-1)).to(device)\n",
    "    next_observations = torch.FloatTensor(next_observations.reshape(batch_size,seq_len,-1)).to(device)\n",
    "    dones = torch.FloatTensor(dones.reshape(batch_size,seq_len,-1)).to(device)\n",
    "\n",
    "    h_target, c_target = target_q_net.init_hidden_state(batch_size=batch_size, training=True)\n",
    "\n",
    "    q_target, _, _ = target_q_net(next_observations, h_target.to(device), c_target.to(device))\n",
    "\n",
    "    q_target_max = q_target.max(2)[0].view(batch_size,seq_len,-1).detach()\n",
    "    targets = rewards + gamma*q_target_max*dones\n",
    "\n",
    "\n",
    "    h, c = q_net.init_hidden_state(batch_size=batch_size, training=True)\n",
    "    q_out, _, _ = q_net(observations, h.to(device), c.to(device))\n",
    "    q_a = q_out.gather(2, actions)\n",
    "\n",
    "    # Multiply Importance Sampling weights to loss        \n",
    "    loss = F.smooth_l1_loss(q_a, targets)\n",
    "    \n",
    "    # Update Network\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def seed_torch(seed):\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.backends.cudnn.enabled:\n",
    "            torch.backends.cudnn.benchmark = False\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def save_model(model, path='default.pth'):\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Env parameters\n",
    "    model_name = \"DRQN\"\n",
    "    env_name = \"SALOHA\"\n",
    "    seed = 1\n",
    "    exp_num = 'SEED'+'_'+str(seed)\n",
    "\n",
    "    # Set gym environment\n",
    "    # env = gym.make(\"CartPole-v1\")\n",
    "    env = PNDEnv()\n",
    "    # env = FlattenObservation(env)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "\n",
    "    # Set the seed\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # seed_torch(seed)\n",
    "    # env.seed(seed)\n",
    "\n",
    "    # default `log_dir` is \"runs\" - we'll be more specific here\n",
    "    # writer = SummaryWriter('runs/'+env_name+\"_\"+model_name+\"_\"+exp_num)\n",
    "    writer = SummaryWriter(comment=\"_\"+env_name+\"_\"+model_name+\"_\"+exp_num)\n",
    "\n",
    "    # Set parameters\n",
    "    batch_size = 8\n",
    "    learning_rate = 1e-3\n",
    "    buffer_len = int(100000)\n",
    "    min_epi_num = 16 # Start moment to train the Q network\n",
    "    episodes = 650\n",
    "    print_per_iter = 20\n",
    "    target_update_period = 4\n",
    "    eps_start = 0.1\n",
    "    eps_end = 0.001\n",
    "    eps_decay = 0.995\n",
    "    tau = 1e-2\n",
    "    max_step = 2000\n",
    "\n",
    "    # DRQN param\n",
    "    random_update = True# If you want to do random update instead of sequential update\n",
    "    lookup_step = 10 # If you want to do random update instead of sequential update\n",
    "    max_epi_len = 128\n",
    "    max_epi_step = max_step\n",
    "\n",
    "    \n",
    "\n",
    "    # Create Q functions\n",
    "    Q = Q_net(state_space=5, # env.observation_space.shape[0]\n",
    "              action_space=env.action_space.n).to(device)\n",
    "    Q_target = Q_net(state_space=5, \n",
    "                     action_space=env.action_space.n).to(device)\n",
    "\n",
    "    Q_target.load_state_dict(Q.state_dict())\n",
    "\n",
    "    # Set optimizer\n",
    "    score = 0\n",
    "    score_sum = 0\n",
    "    optimizer = optim.Adam(Q.parameters(), lr=learning_rate)\n",
    "\n",
    "    epsilon = eps_start\n",
    "    \n",
    "    episode_memory = EpisodeMemory(random_update=random_update, \n",
    "                                   max_epi_num=100, max_epi_len=600, \n",
    "                                   batch_size=batch_size, \n",
    "                                   lookup_step=lookup_step)\n",
    "\n",
    "    # Train\n",
    "    for i in range(episodes):\n",
    "        s, _ = env.reset(seed=seed)\n",
    "        obs = s\n",
    "        done = False\n",
    "        \n",
    "        episode_record = EpisodeBuffer()\n",
    "        h, c = Q.init_hidden_state(batch_size=batch_size, training=False)\n",
    "\n",
    "        for t in range(max_step):\n",
    "\n",
    "            # Get action\n",
    "            a, h, c = Q.sample_action(torch.from_numpy(obs).float().to(device).unsqueeze(0).unsqueeze(0), \n",
    "                                      h.to(device), \n",
    "                                      c.to(device),\n",
    "                                      epsilon)\n",
    "\n",
    "            # Do action\n",
    "            s_prime, r, done, _, _ = env.step(a)\n",
    "            obs_prime = s_prime\n",
    "\n",
    "            # make data\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "\n",
    "            episode_record.put([obs, a, r/100.0, obs_prime, done_mask])\n",
    "\n",
    "            obs = obs_prime\n",
    "            \n",
    "            score += r\n",
    "            score_sum += r\n",
    "\n",
    "            if len(episode_memory) >= min_epi_num:\n",
    "                train(Q, Q_target, episode_memory, device, \n",
    "                        optimizer=optimizer,\n",
    "                        batch_size=batch_size,\n",
    "                        learning_rate=learning_rate)\n",
    "\n",
    "                if (t+1) % target_update_period == 0:\n",
    "                    # Q_target.load_state_dict(Q.state_dict()) <- navie update\n",
    "                    for target_param, local_param in zip(Q_target.parameters(), Q.parameters()): # <- soft update\n",
    "                            target_param.data.copy_(tau*local_param.data + (1.0 - tau)*target_param.data)\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_memory.put(episode_record)\n",
    "        \n",
    "        epsilon = max(eps_end, epsilon * eps_decay) #Linear annealing\n",
    "\n",
    "        if i % print_per_iter == 0 and i!=0:\n",
    "            print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
    "                                                            i, score_sum/print_per_iter, len(episode_memory), epsilon*100))\n",
    "            score_sum=0.0\n",
    "            save_model(Q, model_name+\"_\"+exp_num+'.pth')\n",
    "\n",
    "        # Log the reward\n",
    "        writer.add_scalar('Rewards per episodes', score, i)\n",
    "        score = 0\n",
    "        \n",
    "    writer.close()\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-cert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
