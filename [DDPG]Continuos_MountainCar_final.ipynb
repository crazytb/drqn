{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TqW7cp17LiM"
      },
      "source": [
        "### 패키지 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtWEA6fT6Rr_"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb \\\n",
        "    xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet\n",
        "\n",
        "## gym\n",
        "!pip install gym[classic_control]\n",
        "\n",
        "##ffmpeg\n",
        "!sudo apt-get install ffmpeg -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaDcvqah7TKP"
      },
      "source": [
        "### Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tNDzpMbB7Om5"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "from base64 import b64encode\n",
        "from glob import glob\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipy_display\n",
        "from gym import logger as gym_logger\n",
        "from gym.wrappers.record_video import RecordVideo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rNqWYvgA7Ok3"
      },
      "outputs": [],
      "source": [
        "#### show video func\n",
        "def show_video(mode='train', filename=None):\n",
        "    mp4_list = glob(mode+'/*.mp4')\n",
        "    # print(mp4_list)\n",
        "    if mp4_list:\n",
        "        if filename :\n",
        "            file_lists = glob(mode+'/'+filename)\n",
        "            if not file_lists:\n",
        "                print('No {} found'.format(filename))\n",
        "                return -1\n",
        "            mp4 = file_lists[0]\n",
        "                    \n",
        "        else:\n",
        "            mp4 = sorted(mp4_list)[-1]\n",
        "\n",
        "        print(mp4)\n",
        "        video = open(mp4, 'r+b').read()\n",
        "        encoded = b64encode(video)\n",
        "        ipy_display.display(HTML(data='''\n",
        "            <video alt=\"gameplay\" autoplay controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,%s\" type=\"video/mp4\" />\n",
        "            </video>\n",
        "        ''' % (encoded.decode('ascii'))))\n",
        "    else:\n",
        "        print('No video found')\n",
        "        return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n2QB-vRR7Oij"
      },
      "outputs": [],
      "source": [
        "## save them to file if done\n",
        "def plot_result(save_epi_score):\n",
        "    plt.plot(save_epi_score)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQBRIhVt7eZe"
      },
      "source": [
        "### DDPG Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B9wKpWw57eHk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import gym\n",
        "import random\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-paQ0rQO7eFH"
      },
      "outputs": [],
      "source": [
        "class DDPGagent():\n",
        "    def __init__(self, state_size, action_size, max_action):\n",
        "        # 상태 및 행동 크기 정의\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.action_bound = max_action\n",
        "\n",
        "        ## hyperparameters\n",
        "        self.gamma = 0.95\n",
        "        self.batch_size = 32\n",
        "\n",
        "        # 리플레이 버퍼 크기 및 학습 시작 크기 정의\n",
        "        self.buffer_size = 20000\n",
        "        self.buffer_size_train_start = 2000\n",
        "\n",
        "        self.buffer = deque(maxlen=self.buffer_size)\n",
        "\n",
        "\n",
        "        ## NN Network\n",
        "        self.actor = self.actor_network()\n",
        "        self.target_actor = self.actor_network()\n",
        "\n",
        "        self.critic = self.critic_network()\n",
        "        self.target_critic = self.critic_network()\n",
        "\n",
        "        self.actor_learning_rate = 0.0001\n",
        "        self.critic_learning_rate = 0.001\n",
        "        self.TAU = 0.001\n",
        "\n",
        "        self.actor_opt =  keras.optimizers.Adam(learning_rate = self.actor_learning_rate, clipnorm=5.0)\n",
        "        self.critic_opt =  keras.optimizers.Adam(learning_rate = self.critic_learning_rate, clipnorm=5.0)\n",
        "\n",
        "\n",
        "        # save the results\n",
        "        self.save_epi_score = []\n",
        "\n",
        "    ## actor \n",
        "    def actor_network(self,): \n",
        "        input_ = keras.layers.Input(shape=(self.state_size))\n",
        "\n",
        "        x = keras.layers.Dense(24, activation='relu')(input_)\n",
        "        x = keras.layers.Dense(64, activation='relu', kernel_initializer=keras.initializers.RandomUniform(-1e-3, 1e-3))(x)\n",
        "        x = keras.layers.Dense(16, activation='tanh', kernel_initializer=keras.initializers.RandomUniform(-1e-3, 1e-3))(x)\n",
        "        action = keras.layers.Dense(self.action_size, kernel_initializer=keras.initializers.RandomUniform(-1e-3, 1e-3))(x)\n",
        "        action=keras.layers.Lambda(lambda x : x * self.action_bound)(action)\n",
        "        ## model\n",
        "        model = keras.models.Model(inputs=[input_], outputs=[action])\n",
        "\n",
        "        return model   \n",
        "\n",
        "    ## critic\n",
        "    def critic_network(self,):\n",
        "        input_state = keras.layers.Input(shape=(self.state_size))\n",
        "        input_action = keras.layers.Input(shape=(self.action_size))\n",
        "\n",
        "        state = keras.layers.Dense(32, activation='relu')(input_state)\n",
        "        action = keras.layers.Dense(32, activation='relu')(input_action)\n",
        "\n",
        "        h = keras.layers.concatenate([state, action], axis=-1)\n",
        "        x = keras.layers.Dense(32, activation='relu')(h)\n",
        "        x = keras.layers.Dense(16, activation='relu')(x)\n",
        "        q_func = keras.layers.Dense(1, activation='relu')(x)\n",
        "        ## model\n",
        "        model = keras.models.Model(inputs=[input_state, input_action], outputs=[q_func])\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "    # 입력받은 상태, 행동, 보상, 다음상태, done flag를 리플레이 버퍼에 축적하는 함수 구현\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        item = (state, action, reward, next_state, done)\n",
        "        self.buffer.append(item)\n",
        "\n",
        "\n",
        "    ## get action\n",
        "    def get_action(self,state, pre_noise):\n",
        "        action = self.actor(tf.convert_to_tensor([state], dtype=tf.float32))\n",
        "        # print(f'action_raw : {action}')\n",
        "        action = action.numpy()[0]\n",
        "        ## noise\n",
        "        noise = self.ou_noise(pre_noise, dim=self.action_size)\n",
        "        # clip continuous action to be within action_bound\n",
        "        action = np.clip(action + noise, -self.action_bound, self.action_bound)\n",
        "\n",
        "        # print(f'action : {action}')\n",
        "        return action , noise\n",
        "\n",
        "\n",
        "\n",
        "      ## Soft update Target network\n",
        "    def update_target_network(self, TAU):\n",
        "        theta = self.actor.get_weights()\n",
        "        target_theta = self.target_actor.get_weights()\n",
        "        for i in range(len(theta)):\n",
        "            target_theta[i] = TAU * theta[i] + (1 - TAU) * target_theta[i]\n",
        "        self.target_actor.set_weights(target_theta)\n",
        "\n",
        "        phi = self.critic.get_weights()\n",
        "        target_phi = self.target_critic.get_weights()\n",
        "        for i in range(len(phi)):\n",
        "            target_phi[i] = TAU * phi[i] + (1 - TAU) * target_phi[i]\n",
        "        self.target_critic.set_weights(target_phi)\n",
        "\n",
        "\n",
        "    ## single gradient update on a single batch data\n",
        "    def critic_learn(self, states, actions, td_targets):\n",
        "        with tf.GradientTape() as tape:\n",
        "            q = self.critic([states, actions], training=True)\n",
        "            loss = tf.reduce_mean(tf.square(q - td_targets))\n",
        "\n",
        "        grads = tape.gradient(loss, self.critic.trainable_variables)\n",
        "        self.critic_opt.apply_gradients(zip(grads, self.critic.trainable_variables))\n",
        "\n",
        "    ## train the actor network\n",
        "    def actor_learn(self, states):\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = self.actor(states, training=True)\n",
        "            critic_q = self.critic([states, actions])\n",
        "            loss = -tf.reduce_mean(critic_q)\n",
        "\n",
        "        grads = tape.gradient(loss, self.actor.trainable_variables)\n",
        "        self.actor_opt.apply_gradients(zip(grads, self.actor.trainable_variables))\n",
        "  \n",
        "    ## Ornstein Uhlenbeck Noise\n",
        "    def ou_noise(self, x, rho=0.15, mu=0, dt=1e-1, sigma=0.2, dim=1):\n",
        "        return x + rho*(mu - x)*dt + sigma*np.sqrt(dt)*np.random.normal(size=dim)\n",
        "\n",
        "\n",
        "    ## computing TD target: y_k = r_k + gamma*Q(x_k+1, u_k+1)\n",
        "    def td_target(self, rewards, q_values, dones):\n",
        "        y_k = np.asarray(q_values)\n",
        "        for i in range(q_values.shape[0]): # number of batch\n",
        "            if dones[i]:\n",
        "                y_k[i] = rewards[i]\n",
        "            else:\n",
        "                y_k[i] = rewards[i] + self.gamma * q_values[i]\n",
        "        return y_k\n",
        "\n",
        "    ## load actor weights\n",
        "    def load_weights(self, path):\n",
        "        self.actor.load_weights(f'./{path}/actor/mountain_car.h5')\n",
        "        self.critic.load_weights(f'./{path}/critic/mountain_car.h5')\n",
        "\n",
        "\n",
        "    ## train\n",
        "    def train_model(self):\n",
        "        ### replay memory 에서 random하게 minibatch 만큼 샘플을 가져옴\n",
        "        mini_batch = random.sample(self.buffer, self.batch_size)\n",
        "        # mini_batch에서 각 아래 정보로 분리하기\n",
        "        states, actions, rewards, next_states, dones = zip(*mini_batch)\n",
        "\n",
        "        # 분리된 정보를 tensor 형태로 변환\n",
        "        states = tf.convert_to_tensor(states)\n",
        "        actions = tf.convert_to_tensor(actions)\n",
        "        rewards = tf.convert_to_tensor(rewards)\n",
        "        next_states = tf.convert_to_tensor(next_states)\n",
        "        # dones를 True False로 바꿀 껀데 tf.float32 실수 형태로 바꿔 주는코드 (1.0 , 0.0)\n",
        "        dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
        "\n",
        "\n",
        "        # predict target Q-values\n",
        "        target_qs = self.target_critic([\n",
        "                                        next_states,   ## next_state (s_prime)\n",
        "                                        self.target_actor(next_states) ## next_action (a_prime)\n",
        "                                      ])\n",
        "        \n",
        "        # compute TD targets\n",
        "        y_i = self.td_target(rewards, target_qs.numpy(), dones)\n",
        "\n",
        "        # train critic using sampled batch\n",
        "        self.critic_learn(states ,   ### state (s)\n",
        "                          actions,   ### action (a)\n",
        "                          y_i )      ## TD target: y_k = r_k + gamma*Q(x_k+1, u_k+1)\n",
        "\n",
        "        # train actor\n",
        "        self.actor_learn(states)\n",
        "\n",
        "        # update both target network\n",
        "        self.update_target_network(self.TAU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQxUbFmI7kBr"
      },
      "source": [
        "### Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEiQ46Bs7jyd",
        "outputId": "14bdf79a-50e8-4468-be56-36f9656f6bef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 1, Success: 0, max_position: 0.00, Time: 999, Reward: -21.75\n",
            "Episode: 2, Success: 0, max_position: 0.14, Time: 999, Reward: -11.92\n",
            "Episode: 3, Success: 1, max_position: 0.45, Time: 893, Reward: 93.30\n"
          ]
        }
      ],
      "source": [
        "ENV_NAME = 'MountainCarContinuous-v0'\n",
        "env = gym.make(ENV_NAME)\n",
        "\n",
        "# 비디오 레코딩\n",
        "env = RecordVideo(env, './train', episode_trigger =lambda episode_number: True )\n",
        "# env.metadata = {'render.modes': ['human', 'ansi']}\n",
        "\n",
        "# MountainCar 환경의 상태와 행동 크기 정의\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.shape[0]\n",
        "max_action = env.action_space.high[0]\n",
        "\n",
        "# print(f'state_size : {state_size} , action_size:{action_size} , max_action: {max_action}')\n",
        "\n",
        "# 위에서 정의한 DDPG 클래스를 활용하여 agent 정의\n",
        "agent = DDPGagent(state_size, action_size, max_action)\n",
        "\n",
        "## 초기값\n",
        "success = 0\n",
        "max_position = -0.4\n",
        "# initial transfer model weights to target model network\n",
        "agent.update_target_network(1.0)\n",
        "\n",
        "num_episode = 300\n",
        "\n",
        "\n",
        "for ep in range(num_episode):\n",
        "      # reset episode\n",
        "    step, time, episode_score, done = 0 ,0, 0, False\n",
        "\n",
        "    max_position = -0.4\n",
        "\n",
        "    # 초기 noise 설정\n",
        "    pre_noise = np.zeros(agent.action_size)\n",
        "\n",
        "    # 환경 reset을 통해 초기 상태 정의\n",
        "    state = env.reset()\n",
        "\n",
        "    while not done:\n",
        "        action , noise = agent.get_action(state, pre_noise) ## actor network로 action 생성 // 다음 step 때 pre_noise를 여기서 생성된 noise로 사용\n",
        "\n",
        "        # observe reward, new_state\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # score\n",
        "        episode_score += reward\n",
        "\n",
        "\n",
        "        ## 보상설계\n",
        "        car_pos = next_state[0]\n",
        "        car_vel = next_state[1]\n",
        "\n",
        "\n",
        "        ## 2차함수로 만들어 속도가 커지게 더큰 리워드를 위치에 따라 받게함\n",
        "        if car_vel > 0:\n",
        "            reward = float(((car_pos+0.5)*20)**2/10+15*car_vel - step/300) \n",
        "        else:\n",
        "            reward = float(((car_pos+0.5)*20)**2/10 - step/300) \n",
        "\n",
        "\n",
        "        ### max position   \n",
        "        if car_pos > max_position:\n",
        "          ## max position\n",
        "          max_position = car_pos \n",
        "\n",
        "        ## 성공 시 success\n",
        "        if car_pos >=  0.45:\n",
        "            reward+=100\n",
        "            success += 1\n",
        "\n",
        "        step+=1\n",
        "\n",
        "        # add transition to replay buffer\n",
        "        train_reward= reward\n",
        "\n",
        "        # 획득된 상태, 행동, 보상, 다음상태, done flag를 리플레이 버퍼에 축적\n",
        "        agent.remember(state, action, reward, next_state, done)\n",
        "\n",
        "\n",
        "        # buffer 크기가 일정 기준 이상 쌓이면 학습 진행\n",
        "        if len(agent.buffer) >= agent.buffer_size_train_start :\n",
        "            agent.train_model()\n",
        "\n",
        "\n",
        "        # update current state\n",
        "        pre_noise = noise\n",
        "        state = next_state\n",
        "        success = success\n",
        "        time += 1\n",
        "\n",
        "    ## display rewards every episode\n",
        "    print(f'Episode: {ep+1}, Success: {success}, max_position: {max_position :.2f}, Time: {time}, Reward: {episode_score :.2f}')\n",
        "\n",
        "    agent.save_epi_score.append(episode_score)\n",
        "\n",
        "    ## save weights every episode\n",
        "    #print('Now save')\n",
        "    save_path = './save_weights'\n",
        "    try:\n",
        "        os.makedirs(f'{save_path}/critic')\n",
        "        os.makedirs(f'{save_path}/actor')\n",
        "        print(\"make folder\")\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "    agent.actor.save_weights(f\"{save_path}/actor/mountain_car.h5\")\n",
        "    agent.critic.save_weights(f\"{save_path}/critic/mountain_car.h5\")\n",
        "\n",
        "\n",
        "np.savetxt('./save_weights/mountain_car_epi_reward.txt', agent.save_epi_score)\n",
        "print(agent.save_epi_score)\n",
        "\n",
        "\n",
        "plot_result(agent.save_epi_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STWOLWMq7eC_"
      },
      "outputs": [],
      "source": [
        "### max episode\n",
        "### nan이 젤 큰값이므로 이값을제거하고 계산함\n",
        "episode=np.argmax(agent.save_epi_score)\n",
        "# episode=4\n",
        "filename = 'rl-video-episode-{}.mp4'.format(episode)\n",
        "print(\"최대 avg : {} ,에피소드 번호 : {}\".format(max(agent.save_epi_score) , episode))\n",
        "show_video(filename=filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrKe72jz-1Eu"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJp9zpsK7eAq"
      },
      "outputs": [],
      "source": [
        "ENV_NAME = 'MountainCarContinuous-v0'\n",
        "env = gym.make(ENV_NAME)\n",
        "# 비디오 레코딩\n",
        "env = RecordVideo(env, './test', episode_trigger =lambda episode_number: True )\n",
        "agent = DDPGagent(env)\n",
        "agent.load_weights('./save_weights/')\n",
        "\n",
        "time = 0\n",
        "state = env.reset()\n",
        "\n",
        "while True:\n",
        "    action = agent.actor(tf.convert_to_tensor([state], dtype=tf.float32)).numpy()[0]\n",
        "    # print(action.shape)\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    time += 1\n",
        "    \n",
        "    if done:\n",
        "       print('Time: ', time, 'Reward: ', reward)\n",
        "       break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wX33otN7Og5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ap8z_uB7Oet"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IIXKpQq7Oc3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X20n6rZA7Oaz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUd62tik7OYl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
